{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand the Problem and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = pd.read_csv(\"Fraud_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in fraud.columns:\n",
    "    uniques = fraud[col].unique()\n",
    "    print(f\"{col:<30}{len(uniques):<30}{', '.join(map(str, uniques[:5]))}\") \n",
    "    # map(str, ...) applies the str function to each element of uniques[:5], converting each value into a string (if it's not already a string).\n",
    "    # It's necessary because the join() function only works with strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud[\"signup_time\"] = pd.to_datetime(fraud[\"signup_time\"])\n",
    "fraud[\"purchase_time\"] = pd.to_datetime(fraud[\"purchase_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipaddress_mapping = pd.read_csv(\"IpAddress_to_Country.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipaddress_mapping.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipaddress_mapping.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(ip, mapping):\n",
    "    # Check which row in ipaddress_mapping matches the given IP\n",
    "    match = mapping[\n",
    "        (mapping[\"lower_bound_ip_address\"] <= ip) & \n",
    "        (mapping[\"upper_bound_ip_address\"] >= ip)\n",
    "    ]\n",
    "    # Return the country if a match is found\n",
    "    return match[\"country\"].iloc[0] if not match.empty else np.nan\n",
    "\n",
    "# Apply the function to each row in fraud\n",
    "fraud[\"country\"] = fraud[\"ip_address\"].apply(lambda ip: get_country(ip, ipaddress_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fraud.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"country\"].isna()][\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff\n",
    "df['tenure_days'] = (df[\"purchase_time\"] - df[\"signup_time\"]).dt.days\n",
    "df['tenure_seconds'] = (df[\"purchase_time\"] - df[\"signup_time\"]).dt.seconds\n",
    "\n",
    "# Signup\n",
    "df['signup_dow'] = df['signup_time'].dt.dayofweek # 0 = Monday and 6 = Sunday # If you'd like the name of the day (e.g., \"Sunday\", \"Monday\"), use .dt.day_name() instead\n",
    "df['signup_hour'] = df['signup_time'].dt.hour\n",
    "df['signup_week'] = df['signup_time'].dt.isocalendar().week # .dt.isocalendar().week (Preferred): Extracts the ISO week number (1–53). This is aligned with the ISO 8601 standard.\n",
    "\n",
    "# Purchase\n",
    "df['purchase_dow'] = df['purchase_time'].dt.dayofweek\n",
    "df['purchase_hour'] = df['purchase_time'].dt.hour\n",
    "df['purchase_week'] = df['purchase_time'].dt.isocalendar().week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"shared_device_user_cnt\"] = df.groupby(\"device_id\")[\"user_id\"].transform('nunique')\n",
    "df[\"shared_device_flag\"] = df[\"shared_device_user_cnt\"].apply(lambda x: 1 if x> 1 else 0)\n",
    "df[\"shared_ip_user_cnt\"] = df.groupby(\"ip_address\")[\"user_id\"].transform('nunique')\n",
    "df[\"shared_ip_flag\"] = df[\"shared_ip_user_cnt\"].apply(lambda x: 1 if x> 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(\"signup_dow\")[\"class\"].mean().reset_index()\n",
    "\n",
    "day_of_week_map = {\n",
    "    0: \"Sunday\",\n",
    "    1: \"Monday\",\n",
    "    2: \"Tuesday\",\n",
    "    3: \"Wednesday\",\n",
    "    4: \"Thursday\",\n",
    "    5: \"Friday\",\n",
    "    6: \"Saturday\"\n",
    "}\n",
    "\n",
    "data[\"signup_dow\"] = data[\"signup_dow\"].map(day_of_week_map)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "sns.barplot(data=data, x=\"signup_dow\", y=\"class\")\n",
    "ax.axhline(y=df[\"class\"].mean(), linestyle='--', color='r', linewidth=2)\n",
    "\n",
    "for p in ax.patches:\n",
    "    percentage = f\"{p.get_height()*100:.2f}%\"\n",
    "    ax.text(\n",
    "        x=p.get_x() + p.get_width()/2,\n",
    "        y=p.get_height() + 0.001,\n",
    "        ha=\"center\",\n",
    "        s=percentage,\n",
    "        fontsize=12\n",
    "    )\n",
    "ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(\"purchase_dow\")[\"class\"].mean().reset_index()\n",
    "\n",
    "day_of_week_map = {\n",
    "    0: \"Sunday\",\n",
    "    1: \"Monday\",\n",
    "    2: \"Tuesday\",\n",
    "    3: \"Wednesday\",\n",
    "    4: \"Thursday\",\n",
    "    5: \"Friday\",\n",
    "    6: \"Saturday\",\n",
    "}\n",
    "\n",
    "data[\"purchase_dow\"] = data[\"purchase_dow\"].map(day_of_week_map)\n",
    "overall_avg = df[\"class\"].mean()\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax = sns.barplot(data=data, x=\"purchase_dow\", y=\"class\")\n",
    "\n",
    "ax.axhline(\n",
    "    y=overall_avg,\n",
    "    ls=\"--\",\n",
    "    lw=2,\n",
    "    color=\"r\",\n",
    "    label=f\"Average: {overall_avg*100:.2f}%\",\n",
    ")\n",
    "\n",
    "ax.legend(\n",
    "    loc=\"center left\", # Specifies the reference point of the legend within the bbox_to_anchor bounding box. # \"upper right\", \"lower left\", \"best\", etc.\n",
    "    bbox_to_anchor=(1.01, 0.5), # 1.01: Places the legend just outside the right edge of the plot (slightly offset to the right of the axes by 1% of the figure width).\n",
    "    borderaxespad=0, #Controls the padding (in fractional axes coordinates) between the legend and the axes frame.\n",
    "    frameon=False, #Determines whether the legend has a surrounding border (frame).\n",
    ")\n",
    "\n",
    "ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "for p in ax.patches:\n",
    "    percentage = f\"{p.get_height()*100:.2f}%\"\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,\n",
    "        p.get_height() + 0.001,\n",
    "        percentage,\n",
    "        ha=\"center\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(\"source\")[\"class\"].mean().reset_index()\n",
    "\n",
    "overall_avg = df[\"class\"].mean()\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax = sns.barplot(data=data, x=\"source\", y=\"class\")\n",
    "\n",
    "ax.axhline(\n",
    "    y=overall_avg,\n",
    "    ls=\"--\",\n",
    "    lw=2,\n",
    "    color=\"r\",\n",
    "    label=f\"Average: {overall_avg*100:.2f}%\",\n",
    "    alpha=0.4,\n",
    ")\n",
    "\n",
    "ax.legend(\n",
    "    loc=\"center left\", # Specifies the reference point of the legend within the bbox_to_anchor bounding box. # \"upper right\", \"lower left\", \"best\", etc.\n",
    "    bbox_to_anchor=(1.01, 0.5), # bounding box to anchor: 1.01: Places the legend just outside the right edge of the plot (slightly offset to the right of the axes by 1% of the figure width).\n",
    "    borderaxespad=0, # Controls the padding (in fractional axes coordinates) between the legend and the axes frame.\n",
    "    frameon=False, # Determines whether the legend has a surrounding border (frame).\n",
    "    # shadow=True, # Adds a shadow behind the legend.\n",
    "    # fancybox=True, # Controls whether the legend has rounded corners.\n",
    "    # framealpha=0.5, # Sets the transparency of the legend’s frame.\n",
    "    # edgecolor=\"blue\", # Sets the color of the legend’s frame.\n",
    "    # facecolor=\"lightgrey\",  # Sets a light grey background for the legend\n",
    "    # markerscale=1.5, # Enlarges the markers in the legend\n",
    "    # handlelength=3.0,  # Increases handle length\n",
    "    # labelspacing=0.2,  # Reduces spacing between entries\n",
    ")\n",
    "\n",
    "ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "for p in ax.patches:\n",
    "    percentage = f\"{p.get_height()*100:.2f}%\"\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2,\n",
    "        p.get_height() + 0.001,\n",
    "        percentage,\n",
    "        ha=\"center\",\n",
    "        fontsize=12,\n",
    "        \n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,4))\n",
    "sns.boxplot(data=df, x='tenure_seconds')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### a. country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Practice for Decision Trees**\n",
    "\n",
    "- Low Cardinality: Use Label Encoding or One-Hot Encoding.\n",
    "- High Cardinality: Use Target Encoding or Frequency Encoding.\n",
    "- Tree Models: Prefer Label Encoding, as they handle categorical splits efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are many unique countries (e.g., >50), **Target Encoding** or **Frequency Encoding** is preferred. **One-Hot Encoding** should be avoided due to the \"curse of dimensionality.\"\n",
    "\n",
    "**Target Encoding**\n",
    "- Replace each country with the mean of the target variable (class) for that country.\n",
    "- This approach can capture the relationship between a country and the likelihood of fraud.\n",
    "\n",
    "- Benefits\n",
    "    - Captures how likely a country is associated with the target variable (e.g., fraud).\n",
    "    - Reduces dimensionality compared to One-Hot Encoding.\n",
    "- Caution\n",
    "    - May lead to data leakage if applied on the entire dataset. Use it carefully with proper train-test splitting and cross-validation.\n",
    "\n",
    "**Step-by-Step Process for Target Encoding with Train-Test Splitting**\n",
    "1. Split Your Data: First, split your dataset into training and testing sets. Ensure the test set is completely unseen during the encoding process.\n",
    "2. Compute Encoding on the Training Set: For the `country` column, compute the mean of the target variable (`class`) in the training set. Use this mapping to encode the training and test data.\n",
    "3. Apply the Encoding: \n",
    "    - Replace the country column in both training and test sets using the computed encoding_map.\n",
    "    - For unseen categories in the test set (i.e., categories that do not exist in the training set), assign a default value (e.g., the overall mean of the target in the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the encoding map from training data\n",
    "encoding_map = train_df.groupby('country')['class'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = train_df['class'].mean()  # Fallback value: global mean of the target variable in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encoding to the training set\n",
    "train_df['country_encoded'] = train_df['country'].map(encoding_map).fillna(default_value)\n",
    "# Apply encoding to the test set (handle NaN and unseen categories)\n",
    "test_df['country_encoded'] = test_df['country'].map(encoding_map).fillna(default_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation for source**\n",
    "\n",
    "- Use One-Hot Encoding if you’re using linear models or want to avoid any ordinal relationships.\n",
    "- Use Label Encoding if you’re using tree-based models (like Decision Trees, Random Forest, or XGBoost) for simplicity and efficiency.\n",
    "- Use Target Encoding if source has a strong correlation with the target variable and you're confident in avoiding data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df['source_encoded'] = le.fit_transform(train_df['source'])\n",
    "\n",
    "# Apply the same encoding to the test set\n",
    "test_df['source_encoded'] = le.transform(test_df['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Considerations**\n",
    "- browser has moderate cardinality (e.g., Chrome, Safari, Firefox, etc.).\n",
    "- Encoding options depend on the number of unique categories and how you want to handle the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df['browser_encoded'] = le.fit_transform(train_df['browser'])\n",
    "\n",
    "# Apply the same encoding to the test set\n",
    "test_df['browser_encoded'] = le.transform(test_df['browser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Considerations**\n",
    "- sex typically has low cardinality (e.g., Male, Female, Other).\n",
    "- Encoding is straightforward since there are usually only two or three unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df['sex_encoded'] = le.fit_transform(train_df['sex'])\n",
    "\n",
    "# Apply the same encoding to the test set\n",
    "test_df['sex_encoded'] = le.transform(test_df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature = [\n",
    "    \"signup_dow\",\n",
    "    \"signup_week\",\n",
    "    \"signup_hour\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_week\",\n",
    "    \"purchase_hour\",\n",
    "    \"purchase_value\",\n",
    "    \"source_encoded\",\n",
    "    \"browser_encoded\",\n",
    "    \"sex_encoded\",\n",
    "    \"age\",\n",
    "    \"country_encoded\",\n",
    "    \"tenure_seconds\",\n",
    "    \"shared_device_user_cnt\",\n",
    "    \"shared_ip_user_cnt\",\n",
    "]\n",
    "target = \"class\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = train_df[feature] \n",
    "y = train_df[target]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***General Guidelines***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_depth` (Limits the depth of the tree)\n",
    "- Prevents overfitting by controlling the tree's complexity.\n",
    "- Rule of thumb:\n",
    "    - For small datasets (<10,000 samples): Use **max_depth=3-10**.\n",
    "    - For large datasets (>10,000 samples): Experiment with larger values.\n",
    "- Proportional approach:\n",
    "    - Set based on the number of features (**sqrt(n_features)** for classification problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min_samples_split` (Minimum samples required to split a node)\n",
    "- Ensures a split only occurs if enough samples are present, reducing overfitting.\n",
    "- Rule of thumb:\n",
    "    - Set min_samples_split to 2-5% of the dataset size (**int(0.02 * n_samples)**).\n",
    "    - For imbalanced datasets, **adjust according to the minority class size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`min_samples_leaf` (Minimum samples per leaf node)\n",
    "- Ensures that leaf nodes have enough data to make meaningful predictions.\n",
    "- Rule of thumb:\n",
    "    - Use 1-10% of the dataset size (`int(0.01 * n_samples)`).\n",
    "    - For imbalanced datasets, ensure leaf nodes contain enough minority samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For Imbalanced Datasets (0/1 Classes)***\n",
    "\n",
    "When you have an imbalanced dataset (e.g., class 0: 90%, class 1: 10%), ensure that the minority class (class 1) is well-represented in the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_depth**\n",
    "- Prevent deep trees that may overfit the majority class.\n",
    "- Start with smaller depths, such as max_depth=5, and gradually increase while monitoring performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_samples_split**\n",
    "- Set to ensure splits occur only if both classes are represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_minority_samples = df[\"class\"].value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = max(2, int(0.05 * n_minority_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_samples_leaf**\n",
    "- Ensure leaf nodes contain meaningful samples for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = max(1, int(0.01 * n_minority_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=42,\n",
    ")\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Check for Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Evaluate Training and Test Accuracy**\n",
    "- If the training accuracy is high, but the test accuracy is much lower, the model is likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting Sign**\n",
    "- High training accuracy (e.g., 95% or higher).\n",
    "- Significantly lower test accuracy (e.g., below 70%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on training and test data\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.2f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Cross-Validation**\n",
    "- Cross-validation evaluates the model's performance on multiple subsets of the data to check for consistency and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting Sign**\n",
    "- Large variability in cross-validation scores.\n",
    "- Mean cross-validation score significantly lower than training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation Score: {cv_scores.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Analyze Tree Complexity**\n",
    "\n",
    "- Complex trees with deep depths or many nodes are prone to overfitting because they can memorize the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting Sign**\n",
    "- Very deep trees (e.g., depth > 10 for small datasets).\n",
    "- Excessive number of nodes compared to the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access tree attributes\n",
    "tree_depth = tree.tree_.max_depth\n",
    "num_nodes = tree.tree_.node_count\n",
    "\n",
    "print(f\"Tree Depth: {tree_depth}\")\n",
    "print(f\"Number of Nodes: {num_nodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Avoid Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Prune the Tree**\n",
    "- Control the maximum depth, minimum samples per split, or minimum samples per leaf during training.\n",
    "\n",
    "**2. Use Cross-Validation**\n",
    "- Cross-validation ensures the model generalizes well to unseen data and reduces overfitting.\n",
    "\n",
    "**3. Use Regularization Parameters**\n",
    "- ccp_alpha: (Cost-Complexity Pruning)\n",
    "    - Prunes branches that add little predictive power.\n",
    "    - Larger ccp_alpha values result in smaller trees.\n",
    "\n",
    "**4. Use Ensemble Models**\n",
    "- Random Forests or Gradient Boosted Trees reduce overfitting by averaging predictions or using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "\n",
    "# Export the decision tree to Graphviz format\n",
    "dot_data = export_graphviz(\n",
    "    tree,  # The trained DecisionTreeClassifier model\n",
    "    out_file=None,  # No need to save to a file, we handle it in-memory\n",
    "    feature_names=X_train.columns,  # Feature names from the training dataset\n",
    "    class_names=[\"Class 0\", \"Class 1\"],  # Replace with actual class names if available\n",
    "    filled=True,  # Add colors to nodes based on class distribution\n",
    "    rounded=True,  # Round the corners of the nodes\n",
    "    special_characters=True  # Allow special characters in feature names\n",
    ")\n",
    "\n",
    "# Render the Graphviz tree\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "display(graph)\n",
    "# Display the tree in a Jupyter Notebook or save it as a file\n",
    "# graph.render(\"decision_tree\")  # Saves as 'decision_tree.pdf'\n",
    "# graph.view()  # Opens the rendered file in the default viewer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    tree, X_train, y_train, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label=\"Validation Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = tree.feature_importances_\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance for better readability\n",
    "sorted_idx = np.argsort(importance)\n",
    "sorted_importance = importance[sorted_idx]\n",
    "sorted_feature_names = feature_names[sorted_idx]\n",
    "\n",
    "# Create a larger figure for better visualization\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Horizontal bar plot\n",
    "plt.barh(sorted_feature_names, sorted_importance, color=\"skyblue\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Fraud Model Feature Importance\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add gridlines for readability\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust tick parameters for better readability\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best fitted model\n",
    "best_tree = grid_search.best_estimator_\n",
    "# Use the best model to make predictions\n",
    "y_pred = best_tree.predict(X_test)\n",
    "# Evaluate the performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access best_tree attributes\n",
    "best_tree_depth = best_tree.tree_.max_depth\n",
    "num_nodes = best_tree.tree_.node_count\n",
    "\n",
    "print(f\"best_tree Depth: {best_tree_depth}\")\n",
    "print(f\"Number of Nodes: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = best_tree.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort features by importance for better readability\n",
    "sorted_idx = np.argsort(importance)\n",
    "sorted_importance = importance[sorted_idx]\n",
    "sorted_feature_names = feature_names[sorted_idx]\n",
    "\n",
    "# Create a larger figure for better visualization\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Horizontal bar plot\n",
    "plt.barh(sorted_feature_names, sorted_importance, color=\"skyblue\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Fraud Model Feature Importance\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add gridlines for readability\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust tick parameters for better readability\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the decision tree to Graphviz format\n",
    "dot_data = export_graphviz(\n",
    "    best_tree,  # The trained DecisionTreeClassifier model\n",
    "    out_file=None,  # No need to save to a file, we handle it in-memory\n",
    "    feature_names=X_train.columns,  # Feature names from the training dataset\n",
    "    class_names=[\"Class 0\", \"Class 1\"],  # Replace with actual class names if available\n",
    "    filled=True,  # Add colors to nodes based on class distribution\n",
    "    rounded=True,  # Round the corners of the nodes\n",
    "    special_characters=True  # Allow special characters in feature names\n",
    ")\n",
    "\n",
    "# Render the Graphviz tree\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Get predicted probabilities for the train and test sets\n",
    "y_train_proba = best_tree.predict_proba(X_train)[:, 1]  # Probabilities for positive class (1)\n",
    "y_test_proba = best_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
    "auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "# Compute ROC curve and AUC for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "auc_test = roc_auc_score(y_test, y_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train ROC Curve (AUC = {auc_train:.2f})\", color=\"blue\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC Curve (AUC = {auc_test:.2f})\", color=\"green\")\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\", label=\"Random Guess\")  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Train and Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The closer the ROC curve is to the top-left corner, the better the model.\n",
    "- AUC (Area Under Curve):\n",
    "    - Ranges from 0 to 1.\n",
    "    - 0.5: Random guessing.\n",
    "    - 1.0: Perfect classifier.\n",
    "- Diagonal Line:\n",
    "    - Represents random guessing (baseline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the Results**\n",
    "- If the train AUC is much higher than the test AUC:\n",
    "    - The model may be overfitting to the training data.\n",
    "- If both curves are similar:\n",
    "    - The model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train rf_model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rf_model.feature_importances_\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance for better readability\n",
    "sorted_idx = np.argsort(importance)\n",
    "sorted_importance = importance[sorted_idx]\n",
    "sorted_feature_names = feature_names[sorted_idx]\n",
    "\n",
    "# Create a larger figure for better visualization\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Horizontal bar plot\n",
    "plt.barh(sorted_feature_names, sorted_importance, color=\"skyblue\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.title(\"Fraud Model Feature Importance\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add gridlines for readability\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust tick parameters for better readability\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first tree from the Random Forest\n",
    "tree = rf.estimators_[0]\n",
    "\n",
    "# Check the depth of the tree\n",
    "depth = tree.tree_.max_depth\n",
    "print(f\"Depth of the tree: {depth}\")\n",
    "\n",
    "# Check the number of nodes in the tree\n",
    "num_nodes = tree.tree_.node_count\n",
    "print(f\"Number of nodes in the tree: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize one tree from the forest\n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(\n",
    "    rf.estimators_[0],  # Extract the first tree\n",
    "    feature_names=X_train.columns,  # Feature names\n",
    "    class_names=[\"Class 0\", \"Class 1\"],  # Replace with actual class names\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Decision Tree from Random Forest\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "y_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [10, 20, None],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2]\n",
    "# }\n",
    "\n",
    "# # Grid search\n",
    "# grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='roc_auc')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
