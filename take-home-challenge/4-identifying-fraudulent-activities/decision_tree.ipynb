{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_curve,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand the Problem and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = pd.read_csv(\"Fraud_Data.csv\")\n",
    "ipaddress_mapping = pd.read_csv(\"IpAddress_to_Country.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(ip, mapping):\n",
    "    # Check which row in ipaddress_mapping matches the given IP\n",
    "    match = mapping[\n",
    "        (mapping[\"lower_bound_ip_address\"] <= ip) & \n",
    "        (mapping[\"upper_bound_ip_address\"] >= ip)\n",
    "    ]\n",
    "    # Return the country if a match is found\n",
    "    return match[\"country\"].iloc[0] if not match.empty else np.nan\n",
    "\n",
    "# Apply the function to each row in fraud\n",
    "fraud[\"country\"] = fraud[\"ip_address\"].apply(lambda ip: get_country(ip, ipaddress_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fraud.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"signup_time\"] = pd.to_datetime(df[\"signup_time\"])\n",
    "df[\"purchase_time\"] = pd.to_datetime(df[\"purchase_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff\n",
    "df['tenure_days'] = (df[\"purchase_time\"] - df[\"signup_time\"]).dt.days\n",
    "df['tenure_seconds'] = (df[\"purchase_time\"] - df[\"signup_time\"]).dt.seconds\n",
    "\n",
    "# Signup\n",
    "df['signup_dow'] = df['signup_time'].dt.dayofweek # 0 = Monday and 6 = Sunday # If you'd like the name of the day (e.g., \"Sunday\", \"Monday\"), use .dt.day_name() instead\n",
    "df['signup_hour'] = df['signup_time'].dt.hour\n",
    "df['signup_week'] = df['signup_time'].dt.isocalendar().week # .dt.isocalendar().week (Preferred): Extracts the ISO week number (1–53). This is aligned with the ISO 8601 standard.\n",
    "\n",
    "# Purchase\n",
    "df['purchase_dow'] = df['purchase_time'].dt.dayofweek\n",
    "df['purchase_hour'] = df['purchase_time'].dt.hour\n",
    "df['purchase_week'] = df['purchase_time'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"shared_device_user_cnt\"] = df.groupby(\"device_id\")[\"user_id\"].transform('nunique')\n",
    "df[\"shared_device_flag\"] = df[\"shared_device_user_cnt\"].apply(lambda x: 1 if x> 1 else 0)\n",
    "df[\"shared_ip_user_cnt\"] = df.groupby(\"ip_address\")[\"user_id\"].transform('nunique')\n",
    "df[\"shared_ip_flag\"] = df[\"shared_ip_user_cnt\"].apply(lambda x: 1 if x> 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the encoding map from training data\n",
    "encoding_map = train_df.groupby('country')['class'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_value = train_df['class'].mean()  # Fallback value: global mean of the target variable in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encoding to the training set\n",
    "train_df['country_encoded'] = train_df['country'].map(encoding_map).fillna(default_value)\n",
    "# Apply encoding to the test set (handle NaN and unseen categories)\n",
    "test_df['country_encoded'] = test_df['country'].map(encoding_map).fillna(default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df['source_encoded'] = le.fit_transform(train_df['source'])\n",
    "\n",
    "# Apply the same encoding to the test set\n",
    "test_df['source_encoded'] = le.transform(test_df['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df['browser_encoded'] = le.fit_transform(train_df['browser'])\n",
    "\n",
    "# Apply the same encoding to the test set\n",
    "test_df['browser_encoded'] = le.transform(test_df['browser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "train_df['sex_encoded'] = le.fit_transform(train_df['sex'])\n",
    "\n",
    "# Apply the same encoding to the test set\n",
    "test_df['sex_encoded'] = le.transform(test_df['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature = [\n",
    "    \"signup_dow\",\n",
    "    \"signup_week\",\n",
    "    \"signup_hour\",\n",
    "    \"purchase_dow\",\n",
    "    \"purchase_week\",\n",
    "    \"purchase_hour\",\n",
    "    \"purchase_value\",\n",
    "    \"source_encoded\",\n",
    "    \"browser_encoded\",\n",
    "    \"sex_encoded\",\n",
    "    \"age\",\n",
    "    \"country_encoded\",\n",
    "    \"tenure_seconds\",\n",
    "    \"shared_device_user_cnt\",\n",
    "    \"shared_ip_user_cnt\",\n",
    "]\n",
    "target = \"class\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = train_df[feature] \n",
    "y = train_df[target]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. General Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_depth` (Limits the depth of the tree)\n",
    "- Prevents overfitting by controlling the tree's complexity.\n",
    "- Rule of thumb:\n",
    "    - For small datasets (<10,000 samples): Use **max_depth=3-10**.\n",
    "    - For large datasets (>10,000 samples): Experiment with larger values.\n",
    "- Proportional approach:\n",
    "    - Set based on the number of features (**sqrt(n_features)** for classification problems).\n",
    "\n",
    "`min_samples_split` (Minimum samples required to split a node)\n",
    "- Ensures a split only occurs if enough samples are present, reducing overfitting.\n",
    "- Rule of thumb:\n",
    "    - Set min_samples_split to 2-5% of the dataset size (**int(0.02 * n_samples)**).\n",
    "    - For imbalanced datasets, **adjust according to the minority class size**.\n",
    "\n",
    "`min_samples_leaf` (Minimum samples per leaf node)\n",
    "- Ensures that leaf nodes have enough data to make meaningful predictions.\n",
    "- Rule of thumb:\n",
    "    - Use 1-10% of the dataset size (`int(0.01 * n_samples)`).\n",
    "    - For imbalanced datasets, ensure leaf nodes contain enough minority samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. For Imbalanced Datasets (0/1 Classes)\n",
    "\n",
    "When you have an imbalanced dataset (e.g., class 0: 90%, class 1: 10%), ensure that the minority class (class 1) is well-represented in the splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_depth**\n",
    "- Prevent deep trees that may overfit the majority class.\n",
    "- Start with smaller depths, such as max_depth=5, and gradually increase while monitoring performance.\n",
    "\n",
    "**min_samples_split**\n",
    "- Set to ensure splits occur only if both classes are represented\n",
    "\n",
    "**min_samples_leaf**\n",
    "- Ensure leaf nodes contain meaningful samples for both classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Additional Hyperparameters to Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**criterion (Splitting Criterion)**\n",
    "- Defines how the split quality is measured.\n",
    "- Options:\n",
    "    - \"gini\" (default): Gini Impurity.\n",
    "    - \"entropy\": Information Gain.\n",
    "\n",
    "**max_features (Number of Features to Consider for Splits)**\n",
    "\n",
    "- Limits the number of features to consider at each split.\n",
    "- Options:\n",
    "    - \"sqrt\": Square root of the total number of features (common for classification).\n",
    "    - \"log2\": Logarithm base 2 of total features.\n",
    "    - None: All features.\n",
    "\n",
    "**min_weight_fraction_leaf (Minimum Weighted Fraction of Samples in a Leaf)**\n",
    "\n",
    "- Forces a minimum fraction of the weighted input samples in a leaf node.\n",
    "- Useful for datasets with weighted samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv. Considerations for Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Avoid Overfitting**\n",
    "- Limit tree depth using max_depth.\n",
    "- Set a minimum number of samples per leaf (min_samples_leaf) or split (min_samples_split).\n",
    "\n",
    "**2. Improve Generalization**\n",
    "- Use max_features to reduce the chance of overfitting specific features.\n",
    "- Limit the number of leaf nodes (max_leaf_nodes) to simplify the tree.\n",
    "\n",
    "**3. Optimize for Imbalanced Datasets**\n",
    "- Use min_samples_leaf to ensure that leaf nodes have enough samples from minority classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Keep in mind that a very large param_grid can lead to long search times. Start with fewer combinations and refine the grid iteratively based on results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5 \n",
    "n_minority_samples = df[\"class\"].value_counts()[1]\n",
    "min_samples_split = max(2, int(0.05 * n_minority_samples))\n",
    "min_samples_leaf = max(1, int(0.01 * n_minority_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"max_depth\": [3, 5, 10, 20, None],\n",
    "    \"min_samples_split\": stats.randint(2, 50),\n",
    "    \"min_samples_leaf\": stats.randint(1, 20),\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to test\n",
    "    scoring=\"roc_auc\",  # Use a metric suitable for imbalanced data\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model from RandomizedSearchCV\n",
    "best_tree = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to make predictions\n",
    "y_pred = best_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the decision tree to Graphviz format\n",
    "dot_data = export_graphviz(\n",
    "    best_tree,  # The trained DecisionTreeClassifier model\n",
    "    out_file=None,  # No need to save to a file, we handle it in-memory\n",
    "    feature_names=X_train.columns,  # Feature names from the training dataset\n",
    "    class_names=[\"Class 0\", \"Class 1\"],  # Replace with actual class names if available\n",
    "    filled=True,  # Add colors to nodes based on class distribution\n",
    "    rounded=True,  # Round the corners of the nodes\n",
    "    special_characters=True  # Allow special characters in feature names\n",
    ")\n",
    "\n",
    "# Render the Graphviz tree\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access tree attributes\n",
    "tree_depth = best_tree.tree_.max_depth\n",
    "num_nodes = best_tree.tree_.node_count\n",
    "\n",
    "print(f\"Tree Depth: {tree_depth}\")\n",
    "print(f\"Number of Nodes: {num_nodes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of features sorted by importance\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"skyblue\")\n",
    "plt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance - Best Model from Randomized Search\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation that illustrates the performance of a classification model at various thresholds. It is commonly used for binary classification problems to evaluate the **trade-off between sensitivity (True Positive Rate) and specificity (False Positive Rate)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True Positives: Correctly identified fraudulent transactions.\n",
    "- False Positives: Legitimate transactions mistakenly flagged as fraudulent.\n",
    "- The ROC curve helps you visualize the trade-off between catching fraud (TPR) and wrongly flagging legitimate transactions (FPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The closer the ROC curve is to the top-left corner, the better the model.\n",
    "- AUC (Area Under Curve):\n",
    "    - Ranges from 0 to 1.\n",
    "    - 0.5: Random guessing.\n",
    "    - 1.0: Perfect classifier.\n",
    "- Diagonal Line:\n",
    "    - Represents random guessing (baseline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the train and test sets\n",
    "y_train_proba = best_tree.predict_proba(X_train)[:, 1]  # Probabilities for positive class (1)\n",
    "y_test_proba = best_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
    "auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "# Compute ROC curve and AUC for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "auc_test = roc_auc_score(y_test, y_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train ROC Curve (AUC = {auc_train:.2f})\", color=\"blue\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC Curve (AUC = {auc_test:.2f})\", color=\"green\")\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\", label=\"Random Guess\")  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Train and Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the Results**\n",
    "- If the train AUC is much higher than the test AUC:\n",
    "    - The model may be overfitting to the training data.\n",
    "- If both curves are similar:\n",
    "    - The model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix gives you detailed counts for:\n",
    "- True Positives (TP)\n",
    "- True Negatives (TN)\n",
    "- False Positives (FP)\n",
    "- False Negatives (FN)\n",
    "\n",
    "Use confusion matrices to analyze misclassifications (e.g., whether your model misclassifies the minority class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iv. Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Useful for imbalanced datasets where the positive class is rare.\n",
    "- Shows the trade-off between Precision and Recall at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_tree.predict_proba(X_test)[:, 1]  # Only positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights from Your Plot**\n",
    "- High Precision for Low Recall\n",
    "    - At the start of the curve (left side), precision is close to 1.0, indicating the model is very confident about the few predictions it makes at low recall values.\n",
    "    - As recall increases, the model starts to classify more samples as positive, but precision decreases due to an increasing number of false positives.\n",
    "- Sharp Drop in Precision\n",
    "    - The steep decline indicates that as the threshold is lowered, the model includes more false positives, causing precision to degrade rapidly.\n",
    "- Flat at Higher Recall\n",
    "    - At the far right, the curve flattens out, indicating that recall is maximized (all positives are classified as positive), but precision is low due to many false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use a PR Curve?**\n",
    "- Imbalanced Datasets\n",
    "    - PR curves are particularly useful for imbalanced datasets where the positive class is rare. The ROC curve may give an overly optimistic view of performance.\n",
    "- Focus on False Positives and False Negatives\n",
    "    - If minimizing false positives (e.g., fraud detection) or false negatives (e.g., medical diagnosis) is critical, PR curves give a clearer picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"Average Precision Score: {ap_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v. Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shows how performance changes with increasing training data.\n",
    "- Helps diagnose **overfitting** or **underfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(best_tree, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label=\"Validation Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations**\n",
    "- Fluctuating Training Score\n",
    "    - The training score (blue line) shows significant oscillations as the training set size increases.\n",
    "    - This could be due to variability in the data subsets used during training, especially with smaller training sizes. If the dataset isn't well-balanced or has noisy data, this can happen.\n",
    "- Relatively Stable Validation Score\n",
    "    - The validation score (orange line) remains relatively constant with small improvements as the training size increases.\n",
    "    - This suggests that the model generalizes well and is not significantly overfitting the training data.\n",
    "- Training Score Above Validation Score\n",
    "    - The training score is slightly higher than the validation score, which is typical.\n",
    "    - A large gap between the two would suggest overfitting, but here the gap is small, indicating a good generalization balance.\n",
    "- Close Convergence\n",
    "    - The training and validation scores are close to each other as the training set size increases, suggesting that the model's performance is consistent and not underfitting or overfitting significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Takeaways**\n",
    "\n",
    "- Model is likely performing well\n",
    "    - The training and validation scores are close, and there’s no significant drop in validation score, indicating that the model is learning effectively without overfitting.\n",
    "- Instability in Training Score\n",
    "    - The fluctuations in the training score could point to:\n",
    "        - Data imbalance in the training subsets.\n",
    "        - A need for more consistent cross-validation folds.\n",
    "- Potential Issue with Sampling\n",
    "    - The large fluctuations in the training score may indicate an issue with how training samples are selected. Consider stratified sampling if the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Improve Data Sampling**\n",
    "- Check Data Quality\n",
    "    - Ensure the training data is well-distributed and not imbalanced. If imbalanced, use techniques like oversampling (SMOTE) or undersampling.\n",
    "- Use Stratified Sampling\n",
    "    - Ensure each fold in cross-validation maintains the same class proportions as the overall dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Tune Hyperparameters**\n",
    "Even though best_tree comes from a parameter search, consider adding or refining these\n",
    "- Increase max_depth\n",
    "    - A shallow tree might underfit the training data. Increase depth and observe if training accuracy improves while keeping an eye on overfitting.\n",
    "- Decrease min_samples_split and min_samples_leaf:\n",
    "    - These might be too high, preventing the tree from growing fully. Lower values can increase granularity.\n",
    "- Try Different Scorers\n",
    "    - Accuracy might not always be the best metric. For imbalanced datasets, consider:\n",
    "        - scoring=\"f1\" for a balance between precision and recall.\n",
    "        - scoring=\"roc_auc\" for probability-based classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Use More Data**\n",
    "- Expand the Training Set\n",
    "    - If possible, collect more training data or augment the dataset with similar examples.\n",
    "- Data Augmentation\n",
    "    - For specific use cases (e.g., images, text), apply transformations to create synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Optimize Features**\n",
    "- Feature Engineering\n",
    "    - Add meaningful features, interactions, or transformations (e.g., log, polynomial).\n",
    "- Feature Selection\n",
    "    - Remove unimportant or noisy features. Use feature importances or recursive feature elimination (RFE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Try a More Complex Model**\n",
    "- Boosting Algorithms\n",
    "    - Use Gradient Boosting (e.g., XGBoost, LightGBM) for better generalization.\n",
    "- Bagging Algorithms\n",
    "    - Use Random Forests or Extra Trees for ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Cross-Validation with More Folds**\n",
    "- Increase the number of folds in cross-validation to get a more robust evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_tree, X_train, y_train, cv=20, scoring=\"accuracy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label=\"Validation Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Plot Learning Curve Smoothly**\n",
    "- Use a more evenly distributed set of train_sizes for smoother curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = np.linspace(0.1, 1.0, 10)  # Train sizes from 10% to 100%\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_tree, X_train, y_train, train_sizes=train_sizes, cv=10, scoring=\"accuracy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label=\"Validation Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Analyze Misclassifications**\n",
    "- Evaluate misclassified examples to identify patterns or weaknesses in the model.\n",
    "- Use a confusion matrix to gain insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Change Scoring Metric**\n",
    "- If you care more about Precision or Recall (based on PR curve analysis), change the scoring metric:\n",
    "    - For a balanced focus: scoring=\"f1\".\n",
    "    - To focus on minimizing false negatives: scoring=\"recall\".\n",
    "    - To optimize class separation: scoring=\"roc_auc\".\n",
    "\n",
    "**2. Optimize Cross-Validation**\n",
    "- If your dataset is imbalanced, use stratified k-fold cross-validation to ensure equal class proportions in each fold\n",
    "\n",
    "**3. Adjust the Number of Iterations (n_iter)**\n",
    "- If you have sufficient computation resources, increase n_iter to explore more combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Given your best parameters***\n",
    "\n",
    "- criterion: Keep it as 'gini' since it's categorical.\n",
    "- max_depth=3: Explore slightly larger and smaller values (e.g., [2, 3, 4, 5]).\n",
    "- min_samples_leaf=3: Test a range around this value (e.g., [2, 3, 4, 5]).\n",
    "- min_samples_split=13: Narrow the range (e.g., [10, 13, 15, 18])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "param_distributions = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"min_samples_split\": [5, 8, 10, 15],\n",
    "    \"min_samples_leaf\": [10, 15, 20, 25],\n",
    "    \"criterion\": [\"gini\"],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Reduced iterations for focused search\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to make predictions\n",
    "y_pred = best_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the decision tree to Graphviz format\n",
    "dot_data = export_graphviz(\n",
    "    best_tree,  # The trained DecisionTreeClassifier model\n",
    "    out_file=None,  # No need to save to a file, we handle it in-memory\n",
    "    feature_names=X_train.columns,  # Feature names from the training dataset\n",
    "    class_names=[\"Class 0\", \"Class 1\"],  # Replace with actual class names if available\n",
    "    filled=True,  # Add colors to nodes based on class distribution\n",
    "    rounded=True,  # Round the corners of the nodes\n",
    "    special_characters=True  # Allow special characters in feature names\n",
    ")\n",
    "\n",
    "# Render the Graphviz tree\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# display(graph)\n",
    "graph.render(\"decision_tree\")  # Saves as 'decision_tree.pdf'\n",
    "graph.view()  # Opens the rendered file in the default viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_tree.feature_importances_\n",
    "\n",
    "# Get indices of features sorted by importance\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"skyblue\")\n",
    "plt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance - Best Model from Randomized Search\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the train and test sets\n",
    "y_train_proba = best_tree.predict_proba(X_train)[:, 1]  # Probabilities for positive class (1)\n",
    "y_test_proba = best_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
    "auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "\n",
    "# Compute ROC curve and AUC for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_train, tpr_train, label=f\"Train ROC Curve (AUC = {auc_train:.2f})\", color=\"blue\")\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC Curve (AUC = {auc_test:.2f})\", color=\"green\")\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\", label=\"Random Guess\")  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Train and Test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_tree.predict_proba(X_test)[:, 1]  # Only positive class\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"Average Precision Score: {ap_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = np.linspace(0.1, 1.0, 10)  # Train sizes from 10% to 100%\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_tree, X_train, y_train, train_sizes=train_sizes, cv=10, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label=\"Validation Score\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyze Feature Importance: Identify the most critical features from the feature importance plot, such as tenure_seconds, shared_device_user_cnt, purchase_hour, and country_encoded. These features are likely to provide meaningful patterns for fraud detection.\n",
    "- Inspect the Decision Tree: Extract key decision splits and paths from the decision tree. Look for combinations of conditions where features interact, leading to a fraud classification. For instance, tenure_seconds <= 1.5 combined with shared_device_user_cnt > 1.5 suggests that accounts created quickly and shared across multiple devices are suspicious.\n",
    "- Formulate Combined Rules:\n",
    "    - Combine multiple high-risk feature conditions. For example, rules like \"short account tenure + shared devices\" can signal a potential fraud pattern.\n",
    "    - Analyze value ranges of critical features, such as specific ranges of purchase_hour that might represent unusual purchase times.\n",
    "- Incorporate Business Logic:\n",
    "    - Leverage domain knowledge to enhance the rules. For example, shared devices might indicate account abuse, while transactions occurring during atypical hours could flag suspicious activity.\n",
    "    - Consider the context behind these rules, such as small test payments or accounts created for short-term misuse.\n",
    "- Create Generalized Rule Structures:\n",
    "    - Design rules that combine conditions rather than relying on a single feature. For example, a rule like \"short tenure accounts and purchases from high-risk countries\" can better capture fraud patterns.\n",
    "    - Highlight the potential impact of these rules, such as temporarily flagging or freezing transactions for additional verification.\n",
    "- Ensure Comprehensive Coverage:\n",
    "    - Include multiple conditions from the decision tree to account for various fraud scenarios rather than focusing on isolated features.\n",
    "    - Use different branches and paths from the tree to identify diverse patterns and ensure the rules address a broad spectrum of fraud behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Short Tenure + Shared Device**\n",
    "- Rule: tenure_seconds <= 1.5 AND shared_device_user_cnt > 1.5\n",
    "- Reason: New accounts sharing devices with multiple users are likely fraudulent.\n",
    "- Action: Block the account or require additional identity verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predict_1\"] = np.where(\n",
    "    (df[\"tenure_seconds\"] <= 1) & \n",
    "    (df[\"shared_device_user_cnt\"] >= 2),\n",
    "    1,\n",
    "    0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate True Positives, False Positives, and False Negatives\n",
    "true_positives = len(df[(df[\"class\"] == 1) & (df[\"predict_1\"] == 1)])\n",
    "false_positives = len(df[(df[\"class\"] == 0) & (df[\"predict_1\"] == 1)])\n",
    "false_negatives = len(df[(df[\"class\"] == 1) & (df[\"predict_1\"] == 0)])\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Short Tenure + Unusual Purchase Time**\n",
    "- Rule: tenure_seconds <= 1.5 AND purchase_hour <= 2.5\n",
    "- Reason: Fraudsters often make purchases at odd hours after creating new accounts.\n",
    "- Action: Freeze the transaction or request further verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"tenure_seconds\"] <= 1) & (df[\"purchase_hour\"] <= 2)][\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"tenure_seconds\"] <= 1) & (df[\"purchase_hour\"] <= 2)][\"class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. High-Risk Country + Shared IP**\n",
    "- Rule: country_encoded > 0.052 AND shared_ip_user_cnt > 2.0\n",
    "- Reason: Multiple accounts from the same high-risk IP indicate potential proxy or bot activity.\n",
    "- Action: Trigger additional verification steps, like SMS or ID checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[\"country_encoded\"] > 0.149][\"country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"country\"].isin(['Denmark', 'Ecuador', 'Chile', 'Armenia', 'Honduras', 'Lithuania',\n",
    "       'Ireland', 'Sri Lanka', 'Egypt', 'New Zealand', 'Peru', 'Tunisia',\n",
    "       'Luxembourg', 'Kuwait', 'Senegal', 'Bolivia', 'Namibia', 'Malta',\n",
    "       'Malawi', 'Uzbekistan', 'Afghanistan', 'Turkmenistan'])) & (df[\"shared_ip_user_cnt\"] > 2)][\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Short Tenure + Low Purchase Amount + Odd Purchase Time**\n",
    "- Rule: tenure_seconds <= 1.5 AND purchase_value <= 10.5 AND purchase_hour <= 2.5\n",
    "- Reason: Fraudsters often test stolen payment methods with small transactions at odd hours.\n",
    "- Action: Decline such transactions and investigate the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"tenure_seconds\"] <= 1) & (df[\"purchase_hour\"] <= 2) & (df[\"purchase_value\"] <= 10.5)][\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Unusual Purchase Time + Shared Device + High-Risk Country**\n",
    "- Rule: purchase_hour <= 2.5 AND shared_device_user_cnt > 1.5 AND country_encoded > 0.052\n",
    "- Reason: A combination of unusual time, shared device, and risky location indicates fraud.\n",
    "- Action: Block transactions and escalate for investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"tenure_seconds\"] <= 1) & (df[\"purchase_hour\"] <= 2) & (df[\"purchase_value\"] <= 10.5)][\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Short Tenure + Unusual Signup Day + Shared Device**\n",
    "- Rule: tenure_seconds <= 1.5 AND signup_dow in [1, 3] AND shared_device_user_cnt > 1.5\n",
    "- Reason: Many fake accounts are registered on specific days using shared devices.\n",
    "- Action: Restrict account activities and request additional verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predict_6\"] = np.where(\n",
    "    (df[\"tenure_seconds\"] <= 1) & \n",
    "    (df[\"signup_dow\"].isin([1, 3])) & \n",
    "    (df[\"shared_device_user_cnt\"] >= 2),\n",
    "    1,\n",
    "    0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate True Positives, False Positives, and False Negatives\n",
    "true_positives = len(df[(df[\"class\"] == 1) & (df[\"predict_6\"] == 1)])\n",
    "false_positives = len(df[(df[\"class\"] == 0) & (df[\"predict_6\"] == 1)])\n",
    "false_negatives = len(df[(df[\"class\"] == 1) & (df[\"predict_6\"] == 0)])\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
