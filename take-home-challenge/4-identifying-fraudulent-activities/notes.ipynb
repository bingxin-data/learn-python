{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` is a powerful built-in Python function that applies a given function to every item of an iterable (e.g., list, tuple) and returns a new iterable (a map object).\n",
    "`map(function, iterable)`\n",
    "- `function`: The function you want to apply to each item in the iterable. It can be a built-in function, a user-defined function, or even an anonymous function (lambda).\n",
    "- `iterable`: The iterable (e.g., list, tuple) whose elements will be processed.\n",
    "\n",
    "The `map` function doesn't return a list directly—it returns a `map` object, which you can convert into a list or other data structures as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "str_numbers = map(str, numbers)\n",
    "print(list(str_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "squares = map(square, numbers)\n",
    "print(list(squares))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "squares = map(lambda x: x**2, numbers)\n",
    "print(list(squares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Multiple Iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3]\n",
    "list2 = [4, 5, 6]\n",
    "\n",
    "sums = map(lambda x,y:x+y, list1, list2)\n",
    "print(list(sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing leading and trailing spaces from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"  Hello \", \" World \", \"Python  \"]\n",
    "\n",
    "cleaned_data = map(str.strip, data)\n",
    "print(list(cleaned_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, `strip` is not a standalone function; it's a method that belongs to string objects. That means you can only call `.strip()` on a string instance (e.g., `\"hello\".strip()`), but it doesn’t exist as a global function like `len()` or `print()`.\n",
    "\n",
    "`str.strip` refers to the strip method bound to the str class. When you pass `str.strip` to map, it works because map applies it to each string element in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What If Data Contains Non-Strings?**\n",
    "If your data contains non-string elements, you might run into errors because `.strip()` only works on strings. To handle this, you can combine `str()` conversion with `strip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"  hello  \", None, 42]\n",
    "result = map(lambda x: str(x).strip(), data)  # Convert to string, then strip\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting dates from YYYYMMDD to YYYY-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [20231222, 20240101, 20240214]\n",
    "\n",
    "formatted_dates = map(lambda d: f\"{str(d)[:4]}-{str(d)[4:6]}-{str(d)[6:]}\", dates)\n",
    "print(list(formatted_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing None with 0 in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, None, 4, None, 6]\n",
    "cleaned_data = map(lambda x: 0 if x is None else x, data)\n",
    "print(list(cleaned_data)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Create a Deep Copy?**\n",
    "By default, if you assign one DataFrame to another, both will share the same underlying data. This means any changes you make to the new DataFrame will also reflect in the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = df1  # Not a copy; df2 is just a reference to df1\n",
    "\n",
    "df2['A'] = [10, 20, 30]  # Changes df2\n",
    "print(df1)  # df1 is also changed\n",
    "# Output:\n",
    "#     A  B\n",
    "# 0  10  4\n",
    "# 1  20  5\n",
    "# 2  30  6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, you need to create a copy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `copy()` for a Deep Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.copy()` method to create a new DataFrame that is completely independent of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = df1.copy()  # Creates an independent copy\n",
    "\n",
    "df2['A'] = [10, 20, 30]  # Changes df2\n",
    "print(df1)  # df1 remains unchanged\n",
    "# Output:\n",
    "#    A  B\n",
    "# 0  1  4\n",
    "# 1  2  5\n",
    "# 2  3  6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Only a Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want a subset of the original DataFrame, you can still use `.copy()` to ensure independence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = df1[['A']].copy()  # Copy only column 'A'\n",
    "\n",
    "df2['A'] = [10, 20, 30]  # Changes df2\n",
    "print(df1)  # df1 remains unchanged\n",
    "# Output:\n",
    "#    A  B\n",
    "# 0  1  4\n",
    "# 1  2  5\n",
    "# 2  3  6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens Without `.copy()`?**\n",
    "Without `.copy()`, slicing or selecting columns returns a view of the original DataFrame, not a copy. Modifying this view will also modify the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = df1[['A']]  # No copy, just a view\n",
    "\n",
    "df2['A'] = [10, 20, 30]  # Changes df1 as well!\n",
    "print(df1)\n",
    "# Output:\n",
    "#     A  B\n",
    "# 0  10  4\n",
    "# 1  20  5\n",
    "# 2  30  6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.bincount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.bincount` is a NumPy function that counts the occurrences of each integer value in an array of non-negative integers. It's a fast and efficient way to compute frequency distributions for integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.bincount(x, weights=None, minlength=0)`\n",
    "- x: Input array of non-negative integers.\n",
    "    - This array represents the values for which you want to count occurrences.\n",
    "- weights (optional): Array of the same length as x that assigns weights to each value.\n",
    "    - If provided, instead of counting occurrences, it computes the sum of weights for each unique integer.\n",
    "- minlength (optional): Minimum length of the output array.\n",
    "    - If the maximum value in x is smaller than minlength, the output array is padded with zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input array\n",
    "x = [0, 1, 1, 2, 2, 2, 3]\n",
    "\n",
    "# Count occurrences\n",
    "counts = np.bincount(x)\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Index 0 appears 1 time.\n",
    "- Index 1 appears 2 times.\n",
    "- Index 2 appears 3 times.\n",
    "- Index 3 appears 1 time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input array\n",
    "x = [0, 1, 1, 2, 2, 2, 3]\n",
    "\n",
    "# Weights for each element in x\n",
    "weights = [1, 0.5, 0.5, 2, 2, 2, 3]\n",
    "\n",
    "# Weighted counts\n",
    "weighted_counts = np.bincount(x, weights=weights)\n",
    "print(weighted_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Index 0: Weighted sum = 1\n",
    "- Index 1: Weighted sum = 0.5 + 0.5 = 1\n",
    "- Index 2: Weighted sum = 2 + 2 + 2 = 6\n",
    "- Index 3: Weighted sum = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using minlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 1, 1, 2]\n",
    "\n",
    "# Count occurrences with minlength=5\n",
    "counts = np.bincount(x, minlength=5)\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output array is padded with zeros to ensure a length of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benford's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a dataset (e.g., population data)\n",
    "data = np.random.lognormal(mean=2, sigma=1, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first digits\n",
    "first_digits = [int(str(int(x))[0]) for x in data if x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed distribution\n",
    "observed = np.bincount(first_digits, minlength=10)[1:]  # Exclude 0\n",
    "observed = observed / sum(observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected distribution from Benford's Law\n",
    "expected = [np.log10(1 + 1/d) for d in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, 10), observed, alpha=0.7, label=\"Observed\")\n",
    "plt.plot(range(1, 10), expected, 'ro-', label=\"Expected (Benford's Law)\")\n",
    "plt.xlabel(\"First Digit\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantify Deviations** Use statistical tests to determine if the deviations are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chi-Square Goodness-of-Fit Test:\n",
    "    - Tests whether the observed distribution matches the expected distribution.\n",
    "- Kolmogorov-Smirnov Test:\n",
    "    - Compares the cumulative distribution of observed and expected frequencies.\n",
    "- MAD (Mean Absolute Deviation):\n",
    "    - Measures the average deviation of observed frequencies from expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Square Goodness-of-Fit Test:\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Perform chi-square test\n",
    "chi_stat, p_value = chisquare(f_obs=observed, f_exp=[e * sum(observed) for e in expected])\n",
    "print(\"Chi-Square Statistic:\", chi_stat)\n",
    "print(\"P-Value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret the Results**\n",
    "- Expected Outcome\n",
    "    - If the data follows Benford's Law, the observed distribution of first digits will closely match the expected distribution.\n",
    "\n",
    "- Significant deviations might indicate\n",
    "    - Fraudulent manipulation (e.g., fabricated numbers).\n",
    "    - Data entry errors.\n",
    "    - Specific systematic issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Case: Expense Report Fraud\n",
    "A company suspects that employees might be falsifying expense reports. You can use Benford's Law to check whether **the first digits of expense amounts deviate significantly from the expected distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic smoothing is a technique used to adjust data values dynamically based on specific criteria or conditions. Unlike static smoothing, which applies a fixed formula uniformly to all data points, dynamic smoothing adapts the smoothing intensity depending on context, position, or other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Cases of Sigmoid Smoothing**\n",
    "- Time-Series Data:\n",
    "    - Smooth time-series values while allowing certain intervals (e.g., around nmid) to remain more faithful to the raw data.\n",
    "    - Example: Smoothing temperature data around specific days.\n",
    "- Seasonal Adjustment:\n",
    "    - Gradually smooth noisy data over time to reveal trends while respecting seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate a dataset\n",
    "np.random.seed(42)\n",
    "x = np.arange(1, 101)  # Sequence of 100 points # Context variable (e.g., position, time, or sequence index).\n",
    "y_dow = 10 + 2 * np.sin(x / 10) + np.random.normal(scale=0.5, size=len(x))  # Original data with noise\n",
    "y_avg = np.mean(y_dow)  # Baseline or reference value (e.g., mean).\n",
    "\n",
    "# Sigmoid smoothing parameters\n",
    "nmid = 50  # Transition center point; the position where smoothing starts to change.\n",
    "c = 10  # Controls steepness of the sigmoid curve, a smaller c makes the transition steeper\n",
    "\n",
    "# Apply smoothing formula\n",
    "y_dow_smooth = y_avg + (y_dow - y_avg) / (1 + np.exp(-(x - nmid) / c))\n",
    "\n",
    "# Plot the original and smoothed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y_dow, label=\"Original Data (y_dow)\", alpha=0.7)\n",
    "plt.plot(x, y_dow_smooth, label=\"Smoothed Data (y_dow_smooth)\", linewidth=2, color=\"orange\")\n",
    "plt.axhline(y_avg, color=\"green\", linestyle=\"--\", label=\"Baseline (y_avg)\")\n",
    "plt.axvline(nmid, color=\"red\", linestyle=\"--\", label=\"Smoothing Center (nmid)\")\n",
    "plt.xlabel(\"Index (num)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Sigmoid-Based Smoothing Demonstration\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing Noise in Sparse or Uneven Data**\n",
    "- Problem: Some days of the week may have insufficient data, leading to unreliable or overly noisy target encoding values.\n",
    "    - For example, if Monday has only a few observations, its target mean might not represent its true pattern.\n",
    "- Solution: Sigmoid smoothing balances the raw target mean (from DOW-specific data) with a baseline (global average). This ensures that sparse or noisy days are smoothed toward the baseline while still reflecting the observed trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Edge Cases or Outliers**\n",
    "- Problem: Certain days might have extreme target values due to outliers or anomalies.\n",
    "    - For example, a one-time spike in transactions on Sunday might inflate its target encoding value.\n",
    "- Solution: Sigmoid smoothing down-weights the influence of extreme deviations by blending the raw value with the global average. The gradual transition reduces the risk of overfitting to anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dynamic Smoothing Based on DOW Position**\n",
    "- Problem: Some days may be more important (e.g., weekends for retail sales) and need to retain more of their raw target behavior, while others (e.g., midweek) might require stronger smoothing.\n",
    "- Solution: The sigmoid function adjusts the smoothing intensity dynamically:\n",
    "    - Days near a key reference point (e.g., a midpoint) retain more of their raw target values.\n",
    "    - Days further from the reference point are smoothed more heavily toward the global baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preventing Overfitting**\n",
    "- Problem: When using raw target encoding, the model might overfit to small variations in target values for each DOW.\n",
    "    - For instance, it might assume Sunday is always \"special\" because of a high target mean, even if the sample size is small.\n",
    "- Solution: Sigmoid smoothing regularizes the target encoding, ensuring the model generalizes better by reducing reliance on overly specific DOW trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preserving Global and Local Patterns**\n",
    "- Problem: Balancing the global trend (e.g., overall target average) with local patterns (e.g., DOW-specific means) can be tricky.\n",
    "- Solution: Sigmoid smoothing provides a flexible way to preserve both:\n",
    "    - Raw DOW-specific target means highlight local patterns.\n",
    "    - The global average acts as a stabilizing baseline for days with less data or extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are encoding DOW fraud probabilities, and your data shows:\n",
    "\n",
    "- Weekends (Saturday/Sunday) have high variance.\n",
    "- Weekdays (Monday-Friday) have low variance.\n",
    "\n",
    "\n",
    "- nmin=6, Center the smoothing around Saturday, as weekends are more critical.\n",
    "- c=10, Moderate steepness, balancing transitions across the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is OOT Data?\n",
    "- Definition:\n",
    "\n",
    "    - OOT data is a dataset reserved exclusively for testing or validating a model after it has been trained.\n",
    "    - It is never used during the training phase to ensure the model generalizes well to unseen data.\n",
    "- Difference from Test Data:\n",
    "    - OOT data is sometimes used interchangeably with \"test data,\" but in some cases, it refers to a completely independent dataset collected after the initial training/testing split, such as data from a future time period or a different geographic region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is OOT Data Important?\n",
    "- Prevents Overfitting:\n",
    "    - If the same dataset is used for both training and evaluation, the model may overfit to the training data, leading to overly optimistic performance metrics.\n",
    "- Tests Generalization:\n",
    "    - OOT data provides a realistic test of the model's ability to perform well on new, unseen data.\n",
    "- Validates Real-World Performance:\n",
    "    - OOT data often mimics real-world conditions or future distributions, making it ideal for assessing how the model performs in production.\n",
    "- Detects Data Drift:\n",
    "    - By comparing OOT data to training data, you can identify shifts in data distributions (e.g., seasonality changes, user behavior shifts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is OOT Data Used?\n",
    "- Model Validation:\n",
    "    - Once the model is trained and evaluated on a validation set, OOT data is used to provide a final unbiased estimate of its performance.\n",
    "    - Example: Splitting data into training, validation, and OOT sets.\n",
    "- Monitoring Performance in Production:\n",
    "    - OOT data can simulate real-world data to evaluate whether the model's predictions remain accurate when applied to unseen data.\n",
    "- Model Robustness Testing:\n",
    "    - OOT data can help verify how well the model handles different scenarios or distributions, such as:\n",
    "        - Data **from a different time period**.\n",
    "        - Data **from a different region or demographic**.\n",
    "- Fraud Detection:\n",
    "    - OOT data is often used in fraud detection models to test **whether the model can identify new fraudulent patterns that were not present in the training data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Create OOT Data?\n",
    "- Time-Based Splitting:\n",
    "    - Use a cutoff date to reserve future data for OOT.\n",
    "    - Example:\n",
    "        - **Training: January–June**.\n",
    "        - **OOT: July–December**.\n",
    "- Random Sampling:\n",
    "    - Randomly sample a portion of the dataset (e.g., 20%) as OOT.\n",
    "- Scenario-Based Sampling:\n",
    "    - Create OOT datasets that reflect specific conditions (e.g., **different regions**, **user groups**, or event periods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.argsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.argsort` is a function in the NumPy library that returns the indices that would sort an array. It is useful when you need to know the order of elements in an array without modifying the original array.\n",
    "\n",
    "- a: The input array.\n",
    "- axis:\n",
    "    - Axis along which to sort.\n",
    "    - Default is -1 (last axis).\n",
    "    - Use None to flatten the array before sorting.\n",
    "- kind:\n",
    "    - Sorting algorithm: \"quicksort\", \"mergesort\", \"heapsort\", or \"stable\".\n",
    "    - Default: \"quicksort\".\n",
    "- order: If a is a structured array, this is the field(s) to sort by.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example array\n",
    "a = np.array([10, 3, 7, 1])\n",
    "\n",
    "# Get indices to sort the array\n",
    "sorted_indices = np.argsort(a)\n",
    "print(sorted_indices)  # Output: [3, 1, 2, 0]\n",
    "\n",
    "# Use indices to sort the array\n",
    "sorted_array = a[sorted_indices]\n",
    "print(sorted_array)  # Output: [ 1  3  7 10 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
